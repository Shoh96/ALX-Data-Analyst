{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrangle Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this project is to put into practice what I have learnt in the Data Wrangling Data Course which is part of Udacity Data Analysis Nanodegree program.\n",
    "\n",
    "The dataset I am wrangling is the tweet archive of twitter user WeRateDogs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tasks of this projects are:\n",
    "- Gathering Data\n",
    "- Assessing Data\n",
    "- Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gathering Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were required to gather the data from 3 different sources:\n",
    "\n",
    "- Twitter Archive file:\n",
    "    > This archive contaibs the basic tweet data (tweet ID, timstamp, text, etc) for all 2500+ of their tweets as from August 1, 2017\n",
    "    \n",
    "    > The twitter-archive-enhanced.csv was provided by Udacity, downloaded manually then was loaded from teh CSV file into a pandas data frame\n",
    "\n",
    "- The tweet image predictions:\n",
    "    > This file contains the top predictions of dog breed for each image from the twitter archive. The dataset contains the top predictions, tweet ID, image URL, and the image number corresponding to the most confident prediction.\n",
    "    \n",
    "    > The data is downloaded programmatically using the requests library from the URK addres into a tsv file. The content of image-predictions.tsv file is ten loaded into the pandas data frame.\n",
    "    \n",
    "- Twitter API File:\n",
    "    > This contains the tweet ID, favorite count, retweet count. Data was provided by Udacity, downloaded manually then was loaded from the tweet-json.txt file into a pandas data frame\n",
    "    \n",
    "    > Another way of obtaining the data would have been by requesting permission from twitter and using the python library called tweepy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessing Data describes inspecting the data visually and programmatically. Visual inspection basically involves observing the structure of the data which is a major set-back when dealing with big data. However,the datasets were accessed under two criteria, **Quality** and **Tidiness**.\n",
    "\n",
    "Quality has to do with the content of the data. This includes completeness, validity, accuracy and consistency of a dataset. I was able to identify missing records,incorrect data-type and incorrect data-entries in the different datasets.\n",
    "\n",
    "Tidiness has to do with the structure of the data. In making sure the data was tidy, I corrected the incorrect columns in these datasets and ensured these columns are explicit enough for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality and tidiness issues identified in the Assessing Data section are cleaned using pandas:\n",
    "\n",
    "This is regarded as the final step of the Data Wrangling Process. It contains the Define, Code and Test phase.\n",
    "\n",
    "Before providing solutions to this issues, I created a copy of these datasets and programmatically ensured necessary python packages and libraries were used effectively, I then merged the copies into one dataframe. I defined the solutions to the various quality and tidiness issues, wrote out the codes and tested by checking to see the solutions were properly implemented.\n",
    "\n",
    "Finally, I assigned the cleaned dataframe as 'twitter_archive_master'. This was further used to generate quality insights,deeper analysis and visualization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
